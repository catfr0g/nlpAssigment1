{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "\n",
        "class ContextSensitiveSpellCorrector:\n",
        "    def __init__(self, corpus_path, bigram_path=None):\n",
        "        # Load unigram corpus and build word probability distribution\n",
        "        self.unigram_counts = self.load_corpus(corpus_path)\n",
        "        self.total_words = sum(self.unigram_counts.values())\n",
        "        self.word_probs = {w: c / self.total_words for w, c in self.unigram_counts.items()}\n",
        "        \n",
        "        # Load bigram frequencies if provided; expected format: word1 word2 count\n",
        "        if bigram_path:\n",
        "            self.bigram_counts = self.load_bigrams(bigram_path)\n",
        "        else:\n",
        "            self.bigram_counts = None\n",
        "\n",
        "    def load_corpus(self, path):\n",
        "        \"\"\"Load the text corpus and count word frequencies.\"\"\"\n",
        "        with open(path, 'r') as f:\n",
        "            text = f.read().lower()\n",
        "        words = re.findall(r'\\w+', text)\n",
        "        return Counter(words)\n",
        "    \n",
        "    def load_bigrams(self, path):\n",
        "        \"\"\"Load bigram counts from a file.\"\"\"\n",
        "        bigrams = defaultdict(int)\n",
        "    \n",
        "        with open(path, 'r', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 3:\n",
        "                    bigrams[(parts[1], parts[2])] = int(parts[0])\n",
        "        return bigrams\n",
        "\n",
        "    def edits1(self, word):\n",
        "        \"\"\"Return the set of words that are one edit away from 'word'.\"\"\"\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]            for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:]  for L, R in splits if len(R) > 1]\n",
        "        replaces   = [L + c + R[1:]        for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R            for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "    \n",
        "    def known(self, words):\n",
        "        \"\"\"Filter the set to only include words present in our corpus.\"\"\"\n",
        "        return set(w for w in words if w in self.unigram_counts)\n",
        "\n",
        "    def candidates(self, word):\n",
        "        \"\"\"Generate possible spelling corrections for word.\"\"\"\n",
        "        return (self.known([word]) or \n",
        "                self.known(self.edits1(word)) or \n",
        "                {word})\n",
        "    \n",
        "    def unigram_probability(self, word):\n",
        "        \"\"\"Return the probability of a word from the corpus.\"\"\"\n",
        "        return self.word_probs.get(word, 1e-6)\n",
        "    \n",
        "    def bigram_probability(self, prev, word):\n",
        "        \"\"\"Compute the conditional probability P(word | prev) using bigram counts.\n",
        "           Fallback to unigram probability if bigram data is missing.\"\"\"\n",
        "        if self.bigram_counts:\n",
        "            # Sum over all words that follow the previous word\n",
        "            total_prev = sum(self.bigram_counts[(prev, w)] for w in self.unigram_counts if (prev, w) in self.bigram_counts)\n",
        "            if total_prev > 0:\n",
        "                return self.bigram_counts.get((prev, word), 0) / total_prev\n",
        "        return self.unigram_probability(word)\n",
        "    \n",
        "    def correct_sentence(self, sentence):\n",
        "        \"\"\"Corrects a sentence using context-sensitive bigram probabilities.\"\"\"\n",
        "        tokens = sentence.split()\n",
        "        corrected = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Consider lowercase for matching\n",
        "            token_lower = token.lower()\n",
        "            if i == 0:\n",
        "                # For the first word, use the best candidate by unigram probability.\n",
        "                best = max(self.candidates(token_lower), key=self.unigram_probability)\n",
        "            else:\n",
        "                prev_word = corrected[i - 1]\n",
        "                # For subsequent words, select the candidate maximizing the bigram probability\n",
        "                best = max(self.candidates(token_lower), key=lambda w: self.bigram_probability(prev_word, w))\n",
        "            corrected.append(best)\n",
        "        return \" \".join(corrected)\n",
        "\n",
        "\n",
        "# Paths to your corpus and bigram data\n",
        "corpus_file = \"big.txt\"        # Norvig's corpus (downloaded separately)\n",
        "bigram_file = \"bigrams.txt\"    # Preprocessed bigram data file\n",
        "\n",
        "corrector = ContextSensitiveSpellCorrector(corpus_file, bigram_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: dking sport\n",
            "Corrected: king sport\n",
            "\n",
            "Original: dking species\n",
            "Corrected: king species\n",
            "\n",
            "Original: speling\n",
            "Corrected: spelling\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test sentences: expected \"doing sport\" and \"dying species\"\n",
        "test_sentences = [\n",
        "\t\"dking sport\",\n",
        "\t\"dking species\",\n",
        "\t'speling'\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "\tcorrected = corrector.correct_sentence(sent)\n",
        "\tprint(f\"Original: {sent}\\nCorrected: {corrected}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "*Your text here...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

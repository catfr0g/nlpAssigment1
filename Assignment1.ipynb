{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#imports \n",
        "import re\n",
        "import functools\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "\n",
        "class ContextSensitiveSpellCorrector:\n",
        "    def __init__(self, corpus_path, bigram_path=None):\n",
        "        # Load unigram corpus and build word probability distribution.\n",
        "        self.unigram_counts = self.load_corpus(corpus_path)\n",
        "        self.total_words = sum(self.unigram_counts.values())\n",
        "        self.word_probs = {w: c / self.total_words for w, c in self.unigram_counts.items()}\n",
        "        \n",
        "        # Load bigram frequencies if provided; expected format: count word1 word2\n",
        "        if bigram_path:\n",
        "            self.bigram_counts = self.load_bigrams(bigram_path)\n",
        "            # Precompute totals for each preceding word to avoid repeated summing.\n",
        "            self.bigram_totals = defaultdict(int)\n",
        "            for (prev, word), count in self.bigram_counts.items():\n",
        "                self.bigram_totals[prev] += count\n",
        "        else:\n",
        "            self.bigram_counts = None\n",
        "\n",
        "    def load_corpus(self, path):\n",
        "        \"\"\"Load the text corpus and count word frequencies.\"\"\"\n",
        "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            text = f.read().lower()\n",
        "        pattern = re.compile(r'\\w+')\n",
        "        words = pattern.findall(text)\n",
        "        return Counter(words)\n",
        "    \n",
        "    def load_bigrams(self, path):\n",
        "        \"\"\"Load bigram counts from a file.\"\"\"\n",
        "        bigrams = {}\n",
        "        with open(path, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 3:\n",
        "                    count, w1, w2 = parts\n",
        "                    bigrams[(w1, w2)] = int(count)\n",
        "        return bigrams\n",
        "\n",
        "    @functools.lru_cache(maxsize=None)\n",
        "    def edits1(self, word):\n",
        "        \"\"\"Return the set of words that are one edit away from 'word'.\"\"\"\n",
        "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "    \n",
        "    def known(self, words):\n",
        "        \"\"\"Filter the set to only include words present in our corpus.\"\"\"\n",
        "        return {w for w in words if w in self.unigram_counts}\n",
        "\n",
        "    @functools.lru_cache(maxsize=None)\n",
        "    def candidates(self, word):\n",
        "        \"\"\"Generate possible spelling corrections for a word.\"\"\"\n",
        "        # If the word is known, return it immediately.\n",
        "        if word in self.unigram_counts:\n",
        "            return {word}\n",
        "        # Otherwise, try words that are one edit away.\n",
        "        edits = self.edits1(word)\n",
        "        known_edits = self.known(edits)\n",
        "        return known_edits if known_edits else {word}\n",
        "    \n",
        "    def unigram_probability(self, word):\n",
        "        \"\"\"Return the probability of a word based on unigram counts.\"\"\"\n",
        "        return self.word_probs.get(word, 1e-6)\n",
        "    \n",
        "    def bigram_probability(self, prev, word):\n",
        "        \"\"\"\n",
        "        Compute the conditional probability P(word | prev) using bigram counts.\n",
        "        Fallback to unigram probability if bigram data is missing.\n",
        "        \"\"\"\n",
        "        if self.bigram_counts:\n",
        "            total_prev = self.bigram_totals.get(prev, 0)\n",
        "            if total_prev > 0:\n",
        "                return self.bigram_counts.get((prev, word), 0) / total_prev\n",
        "        return self.unigram_probability(word)\n",
        "    \n",
        "    def correct_sentence(self, sentence):\n",
        "        \"\"\"Correct a sentence using context-sensitive bigram probabilities.\"\"\"\n",
        "        tokens = sentence.split()\n",
        "        corrected = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            token_lower = token.lower()\n",
        "            candidate_set = self.candidates(token_lower)\n",
        "            candidates_list = list(candidate_set)\n",
        "            \n",
        "            # Use numpy to compute probabilities for candidates.\n",
        "            if i == 0:\n",
        "                probs = np.array([self.unigram_probability(c) for c in candidates_list])\n",
        "            else:\n",
        "                prev_word = corrected[i - 1]\n",
        "                probs = np.array([self.bigram_probability(prev_word, c) for c in candidates_list])\n",
        "                \n",
        "            best_idx = np.argmax(probs)\n",
        "            best = candidates_list[best_idx]\n",
        "            corrected.append(best)\n",
        "        return \" \".join(corrected)\n",
        "\n",
        "# Example usage:\n",
        "corpus_file = \"big.txt\"        # Norvig's corpus (downloaded separately)\n",
        "bigram_file = \"bigrams.txt\"    # Preprocessed bigram data file\n",
        "\n",
        "# Initialize the spell corrector.\n",
        "corrector = ContextSensitiveSpellCorrector(corpus_file, bigram_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: dking sport\n",
            "Corrected: king sport\n",
            "\n",
            "Original: dking species\n",
            "Corrected: king species\n",
            "\n",
            "Original: speling\n",
            "Corrected: spelling\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test sentences: expected \"doing sport\" and \"dying species\"\n",
        "test_sentences = [\n",
        "\t\"dking sport\",\n",
        "\t\"dking species\",\n",
        "\t'speling'\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "\tcorrected = corrector.correct_sentence(sent)\n",
        "\tprint(f\"Original: {sent}\\nCorrected: {corrected}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "**Corpus and N-gram Data**\n",
        "\n",
        "Unigram Dataset:\n",
        ">Large unigram corpus (like Norvig’s “big.txt”) is loaded to build word frequency data. This helps rank correction candidates, favoring common words.\n",
        "\n",
        "Bigram Dataset (Context Sensitivity):\n",
        ">If available, we also load bigram frequencies. This allows us to calculate conditional probabilities \\(P(\\text{{word}} \\mid \\text{{prev}})\\), helping the corrector pick words that fit the context, not just the spelling.\n",
        "\n",
        "\n",
        "**Candidate Generation and Edit Distance**\n",
        "\n",
        "Edit Distance 1 Only:\n",
        ">Candidates are generated using the `edits1` function, which finds words one edit away (insertions, deletions, transpositions, replacements). This captures most common typos while keeping computation fast.\n",
        "\n",
        "No Edit Distance 2:\n",
        ">I skip edit distance 2 because it multiplies candidates and slows processing. Instead, algorithm rely on context or treat such words as unknown.\n",
        "\n",
        "**Probability Weights and Smoothing**\n",
        "\n",
        "Unigram and Bigram Probabilities:\n",
        ">Corrector score candidates using unigram or bigram probabilities. If a word isn’t in the corpus, we assign a small default probability (1e-6) to avoid zero values.\n",
        "\n",
        "Implicit Error Weighting: \n",
        ">Corrector don’t assign different weights to edit types. Limiting candidates to one edit naturally favors simpler, more likely corrections.\n",
        "\n",
        "**Efficiency and Caching**\n",
        "\n",
        "LRU Caching:\n",
        ">I use Python’s `functools.lru_cache` for `edits1` and `candidates` to avoid recalculating results for the same word. This speeds up processing, especially for long texts.\n",
        "\n",
        "NumPy for Probability Calculation:\n",
        ">Probabilities are converted to NumPy arrays for faster, vectorized calculations.\n",
        "\n",
        "**Sentence Correction Process**\n",
        "\n",
        "Greedy Selection:  \n",
        ">For each word, corrector pick the candidate with the highest unigram or bigram probability. While beam search could improve coherence, I chose greedy selection for speed and simplicity.\n",
        "\n",
        "Case Normalization:\n",
        ">Code convert tokens to lowercase before generating candidates. This avoids issues from capitalization mismatches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "###CONSTANTS \n",
        "corpus_file = \"big.txt\" \n",
        "bigram_file = \"bigrams.txt\"\n",
        "# Define a list of test sentences (assumed to be lower-case).\n",
        "test_sentences = [\n",
        "    \"this is a smple test sentnce.\",\n",
        "    \"the quick brown fox jumpd over the lazy dog.\",\n",
        "    \"we are experimting with a spel corrector.\",\n",
        "    \"please corect any mispelled words in this sntence.\",\n",
        "    \"anothr exampl of a sentence with erors.\",\n",
        "    \"it is importnt to have accurate spell chcking.\",\n",
        "    \"sometimes computers mispell words too.\",\n",
        "    \"the languge model must be robust and fast.\",\n",
        "    \"she enjoys readng books and writng stories.\",\n",
        "    \"he went to the stroe to by some groceries.\",\n",
        "    \"their are many opportnities in the tech industrty.\",\n",
        "    \"we will evaluat the performance of our spell checker.\",\n",
        "    \"the algoritm behind the spell correction is critcal.\",\n",
        "    \"ensuring proper grammer is essential for clarity.\",\n",
        "    \"he recived a package in the mail yestreday.\",\n",
        "    \"the managment team approved the new plan.\",\n",
        "    \"it is necassary to proofred documents carefully.\",\n",
        "    \"a good speling corrector can improove writting quality.\",\n",
        "    \"our systm detected several errors in the text.\",\n",
        "    \"this exampe is purposely mistaked for evaluation.\",\n",
        "    \"the weather today is unusually cold and windy.\",\n",
        "    \"the study of computr science involves logc and maths.\",\n",
        "    \"programming requires precission and patence.\",\n",
        "    \"the library offers a wide rang of books and articles.\",\n",
        "    \"students are encourged to review their work.\",\n",
        "    \"he always taks notes during lecture.\",\n",
        "    \"the reserch paper was peer reviwed thrughly.\",\n",
        "    \"an effective spell chcker needs a large corpus.\",\n",
        "    \"the output of the model is quite impresive.\",\n",
        "    \"we are curently working on improving the systm.\",\n",
        "    \"the commitee met to discss the new proposal.\",\n",
        "    \"the developrs are integrating new features into the app.\",\n",
        "    \"this sentnce has multiple erors that need corection.\",\n",
        "    \"accuracy in speling is essential for professionalism.\",\n",
        "    \"she recieves many compliments on her writting.\",\n",
        "    \"the officl meeting has been rescheduled for tuesday.\",\n",
        "    \"we need to update the databse with new entries.\",\n",
        "    \"the softwere developer fixed the bug in the code.\",\n",
        "    \"this exampl illustrates the issues in text processing.\",\n",
        "    \"we must implemnt a robust spell chekcer algorithm.\",\n",
        "    \"the user interfac is clear and intuitive.\",\n",
        "    \"analyzing large amounts of data is a challnge.\",\n",
        "    \"the currnt version of the program contains bugs.\",\n",
        "    \"it is important to maintain consistncy in data.\",\n",
        "    \"the report was submitted by the dedicted team.\",\n",
        "    \"the attendence at the conference was impressive.\",\n",
        "    \"the opertion was completed within the time limit.\",\n",
        "    \"new technologies are emerging fast in the industry.\",\n",
        "    \"she has a keen eye for detail and critcal thinking.\",\n",
        "    \"the experience was both enriching and challengng.\",\n",
        "    \"improving our speling corrector will benefit many users.\",\n",
        "    \"the datasheet contains in-depth informations about the product.\",\n",
        "    \"the system performnce is evaluated under various conditions.\",\n",
        "    \"he practised his writng skills daily.\",\n",
        "    \"the course covers a wide range of computtion topics.\",\n",
        "    \"a welldone spell checker can save time and reduce errors.\",\n",
        "    \"the experiment involves both qualitative and quantitive analysis.\",\n",
        "    \"her respons was quick and helpful.\",\n",
        "    \"the survey results were summarised in the report.\",\n",
        "    \"this sentnce intentionally incorrrect to test the system.\",\n",
        "    \"the language model was trained on a verity of data sources.\",\n",
        "    \"spell chekcing is a useful tool in many applications.\",\n",
        "    \"the improvement in performance is noticable.\",\n",
        "    \"our team consist of experinced professionals.\",\n",
        "    \"a detailed analysis was conductd for this study.\",\n",
        "    \"the text corpus provides a rich source of data.\",\n",
        "    \"the webscraper gathered a large number of pages.\",\n",
        "    \"each page was processed and split into sentnces.\",\n",
        "    \"the output list of sentnces will be used for evaluation.\",\n",
        "    \"the algorithm rturns the most probable corrections.\",\n",
        "    \"the sample input contains several mispeled words.\",\n",
        "    \"we use statistical models to rank the canddiates.\",\n",
        "    \"the correction procedure takes context into acount.\",\n",
        "    \"the system architecture was designed for scalabilty.\",\n",
        "    \"machine learning can assist in spell correction.\",\n",
        "    \"the evaluation metric includes both accuracy and recall.\",\n",
        "    \"this test data simulates real-world mistkes.\",\n",
        "    \"it is crucial to have a robust preprocessng pipeline.\",\n",
        "    \"cleaning the text removes unwanted characters and tags.\",\n",
        "    \"the iterative parsing helps manage memory effectively.\",\n",
        "    \"our research focusses on improving error detection.\",\n",
        "    \"the training corpus consists of millions of words.\",\n",
        "    \"the implementtion of the algorithm was sucessful.\",\n",
        "    \"the performance of the spell checker is evaluatd using benchmark tests.\",\n",
        "    \"the results indicate a signficant improvement over previous versions.\",\n",
        "    \"the datacleaning process is essential before analysis.\",\n",
        "    \"the printed report contains various statstics and graphs.\",\n",
        "    \"the programm was developed using python and xml libraries.\",\n",
        "    \"each sample sentence has been curately selected for testing.\",\n",
        "    \"the text has been normalized to lower case and cleaned thoroughly.\",\n",
        "    \"the evaluation process is iterative and data-driven.\",\n",
        "    \"user feedback is important to refine the correction algorithm.\",\n",
        "    \"the model's prediction is compared against a gold standard.\",\n",
        "    \"the sentence segmentation is carried out using nltk.\",\n",
        "    \"the python code integrates download, preprocessing, and extraction.\",\n",
        "    \"the ultimate goal is to achieve high accuracy in spell correcton.\",\n",
        "    \"the test corpus is diverse and covers many topics.\",\n",
        "    \"the refined algorithm shows promise in real-world applications.\",\n",
        "    \"the output of the preprocessing pipeline is stored in a list.\",\n",
        "    \"this final sentence completes our 100 evaluation samples.\"\n",
        "]\n",
        "\n",
        "\n",
        "noise_probs = [0.1, 0.3, 0.5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this is a smple test sentnce.',\n",
              " 'the quick brown fox jumpd over the lazy dog.',\n",
              " 'we are experimting with a spel corrector.',\n",
              " 'please corect any mispelled words in this sntence.',\n",
              " 'anothr exampl of a sentence with erors.',\n",
              " 'it is importnt to have accurate spell chcking.',\n",
              " 'sometimes computers mispell words too.',\n",
              " 'the languge model must be robust and fast.',\n",
              " 'she enjoys readng books and writng stories.',\n",
              " 'he went to the stroe to by some groceries.',\n",
              " 'their are many opportnities in the tech industrty.',\n",
              " 'we will evaluat the performance of our spell checker.',\n",
              " 'the algoritm behind the spell correction is critcal.',\n",
              " 'ensuring proper grammer is essential for clarity.',\n",
              " 'he recived a package in the mail yestreday.',\n",
              " 'the managment team approved the new plan.',\n",
              " 'it is necassary to proofred documents carefully.',\n",
              " 'a good speling corrector can improove writting quality.',\n",
              " 'our systm detected several errors in the text.',\n",
              " 'this exampe is purposely mistaked for evaluation.',\n",
              " 'the weather today is unusually cold and windy.',\n",
              " 'the study of computr science involves logc and maths.',\n",
              " 'programming requires precission and patence.',\n",
              " 'the library offers a wide rang of books and articles.',\n",
              " 'students are encourged to review their work.',\n",
              " 'he always taks notes during lecture.',\n",
              " 'the reserch paper was peer reviwed thrughly.',\n",
              " 'an effective spell chcker needs a large corpus.',\n",
              " 'the output of the model is quite impresive.',\n",
              " 'we are curently working on improving the systm.',\n",
              " 'the commitee met to discss the new proposal.',\n",
              " 'the developrs are integrating new features into the app.',\n",
              " 'this sentnce has multiple erors that need corection.',\n",
              " 'accuracy in speling is essential for professionalism.',\n",
              " 'she recieves many compliments on her writting.',\n",
              " 'the officl meeting has been rescheduled for tuesday.',\n",
              " 'we need to update the databse with new entries.',\n",
              " 'the softwere developer fixed the bug in the code.',\n",
              " 'this exampl illustrates the issues in text processing.',\n",
              " 'we must implemnt a robust spell chekcer algorithm.',\n",
              " 'the user interfac is clear and intuitive.',\n",
              " 'analyzing large amounts of data is a challnge.',\n",
              " 'the currnt version of the program contains bugs.',\n",
              " 'it is important to maintain consistncy in data.',\n",
              " 'the report was submitted by the dedicted team.',\n",
              " 'the attendence at the conference was impressive.',\n",
              " 'the opertion was completed within the time limit.',\n",
              " 'new technologies are emerging fast in the industry.',\n",
              " 'she has a keen eye for detail and critcal thinking.',\n",
              " 'the experience was both enriching and challengng.',\n",
              " 'improving our speling corrector will benefit many users.',\n",
              " 'the datasheet contains in-depth informations about the product.',\n",
              " 'the system performnce is evaluated under various conditions.',\n",
              " 'he practised his writng skills daily.',\n",
              " 'the course covers a wide range of computtion topics.',\n",
              " 'a welldone spell checker can save time and reduce errors.',\n",
              " 'the experiment involves both qualitative and quantitive analysis.',\n",
              " 'her respons was quick and helpful.',\n",
              " 'the survey results were summarised in the report.',\n",
              " 'this sentnce intentionally incorrrect to test the system.',\n",
              " 'the language model was trained on a verity of data sources.',\n",
              " 'spell chekcing is a useful tool in many applications.',\n",
              " 'the improvement in performance is noticable.',\n",
              " 'our team consist of experinced professionals.',\n",
              " 'a detailed analysis was conductd for this study.',\n",
              " 'the text corpus provides a rich source of data.',\n",
              " 'the webscraper gathered a large number of pages.',\n",
              " 'each page was processed and split into sentnces.',\n",
              " 'the output list of sentnces will be used for evaluation.',\n",
              " 'the algorithm rturns the most probable corrections.',\n",
              " 'the sample input contains several mispeled words.',\n",
              " 'we use statistical models to rank the canddiates.',\n",
              " 'the correction procedure takes context into acount.',\n",
              " 'the system architecture was designed for scalabilty.',\n",
              " 'machine learning can assist in spell correction.',\n",
              " 'the evaluation metric includes both accuracy and recall.',\n",
              " 'this test data simulates real-world mistkes.',\n",
              " 'it is crucial to have a robust preprocessng pipeline.',\n",
              " 'cleaning the text removes unwanted characters and tags.',\n",
              " 'the iterative parsing helps manage memory effectively.',\n",
              " 'our research focusses on improving error detection.',\n",
              " 'the training corpus consists of millions of words.',\n",
              " 'the implementtion of the algorithm was sucessful.',\n",
              " 'the performance of the spell checker is evaluatd using benchmark tests.',\n",
              " 'the results indicate a signficant improvement over previous versions.',\n",
              " 'the datacleaning process is essential before analysis.',\n",
              " 'the printed report contains various statstics and graphs.',\n",
              " 'the programm was developed using python and xml libraries.',\n",
              " 'each sample sentence has been curately selected for testing.',\n",
              " 'the text has been normalized to lower case and cleaned thoroughly.',\n",
              " 'the evaluation process is iterative and data-driven.',\n",
              " 'user feedback is important to refine the correction algorithm.',\n",
              " \"the model's prediction is compared against a gold standard.\",\n",
              " 'the sentence segmentation is carried out using nltk.',\n",
              " 'the python code integrates download, preprocessing, and extraction.',\n",
              " 'the ultimate goal is to achieve high accuracy in spell correcton.',\n",
              " 'the test corpus is diverse and covers many topics.',\n",
              " 'the refined algorithm shows promise in real-world applications.',\n",
              " 'the output of the preprocessing pipeline is stored in a list.',\n",
              " 'this final sentence completes our 100 evaluation samples.']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### CODE FROM NORVIG IMPLEMENTATION\n",
        "\n",
        "def words(text): \n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open(corpus_file).read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NorvigFacade:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def correct_sentence(self, sentence):\n",
        "        \"Correct each word in the sentence using the Norvig approach.\"\n",
        "        return \" \".join(correction(word) for word in sentence.split())\n",
        "\n",
        "norvig_corrector = NorvigFacade()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "def add_noise_to_word(word, noise_prob):\n",
        "    \"\"\"With probability noise_prob, perform one random edit on the word.\"\"\"\n",
        "    if random.random() < noise_prob and word:\n",
        "        operations = ['delete', 'insert', 'substitute', 'transpose']\n",
        "        op = random.choice(operations)\n",
        "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        if op == 'delete' and len(word) > 0:\n",
        "            idx = random.randint(0, len(word)-1)\n",
        "            return word[:idx] + word[idx+1:]\n",
        "        elif op == 'insert':\n",
        "            idx = random.randint(0, len(word))\n",
        "            return word[:idx] + random.choice(letters) + word[idx:]\n",
        "        elif op == 'substitute' and len(word) > 0:\n",
        "            idx = random.randint(0, len(word)-1)\n",
        "            return word[:idx] + random.choice(letters) + word[idx+1:]\n",
        "        elif op == 'transpose' and len(word) > 1:\n",
        "            idx = random.randint(0, len(word)-2)\n",
        "            return word[:idx] + word[idx+1] + word[idx] + word[idx+2:]\n",
        "    return word\n",
        "\n",
        "def add_noise(sentence, noise_prob):\n",
        "    \"\"\"Apply noise to every word in the sentence with given probability.\"\"\"\n",
        "    words = sentence.split()\n",
        "    noisy_words = [add_noise_to_word(word, noise_prob) for word in words]\n",
        "    return \" \".join(noisy_words)\n",
        "\n",
        "def evaluate(corrector, test_sentences, noise_prob):\n",
        "    total_words = 0\n",
        "    correct_words = 0\n",
        "    total_sentences = 0\n",
        "    correct_sentences = 0\n",
        "    \n",
        "    for sentence in test_sentences:\n",
        "        noisy_sentence = add_noise(sentence, noise_prob)\n",
        "        corrected = corrector.correct_sentence(noisy_sentence)\n",
        "        \n",
        "        orig_words = sentence.split()\n",
        "        corr_words = corrected.split()\n",
        "        total_words += len(orig_words)\n",
        "        # For word-level, count matching words (compare up to the length of the original)\n",
        "        correct_words += sum(1 for o, c in zip(orig_words, corr_words) if o == c)\n",
        "        \n",
        "        total_sentences += 1\n",
        "        if orig_words == corr_words:\n",
        "            correct_sentences += 1\n",
        "    word_accuracy = correct_words / total_words\n",
        "    sentence_accuracy = correct_sentences / total_sentences\n",
        "    return word_accuracy, sentence_accuracy\n",
        "\n",
        "cs_corrector = ContextSensitiveSpellCorrector(corpus_file, bigram_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation of Spell Correctors on Noisy Test Set:\n",
            "\n",
            "Noise Prob  Context-Sensitive (Word / Sentence)     Norvig (Word / Sentence)                \n",
            "0.10        78.14%               / 7.00%            72.86%               / 2.00%            \n",
            "0.30        73.37%               / 8.00%            64.57%               / 1.00%            \n",
            "0.50        66.33%               / 1.00%            60.30%               / 1.00%            \n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Evaluation of Spell Correctors on Noisy Test Set:\\n\")\n",
        "print(\"{:<12}{:<40}{:<40}\".format(\"Noise Prob\", \"Context-Sensitive (Word / Sentence)\", \"Norvig (Word / Sentence)\"))\n",
        "for noise_prob in noise_probs:\n",
        "    cs_word_acc, cs_sent_acc = evaluate(cs_corrector, test_sentences, noise_prob)\n",
        "    norvig_word_acc, norvig_sent_acc = evaluate(norvig_corrector, test_sentences, noise_prob)\n",
        "    print(\"{:<12.2f}{:<20.2%} / {:<17.2%}{:<20.2%} / {:<17.2%}\".format(\n",
        "        noise_prob, cs_word_acc, cs_sent_acc, norvig_word_acc, norvig_sent_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "my implementaion has a little bit higher metrics than naive implementsion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
